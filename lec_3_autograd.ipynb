{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db2f97",
   "metadata": {},
   "outputs": [],
   "source": [
    " import torch\n",
    "\n",
    " import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "831783c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple's MPS\n",
      "Device is: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple's MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA gpu: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using cpu\")\n",
    "print(f\"Device is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c6d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = x^2. Here x=3, so dy/dx = 2(x), ie 6\n",
      "gradient of y w.r.t x is: 6.0\n"
     ]
    }
   ],
   "source": [
    "# Simple Eg 1\n",
    "x = torch.tensor([3.0], requires_grad = True)\n",
    "y = torch.pow(x,2)\n",
    "print(f\"y = x^2. Here x=3, so dy/dx = 2(x), ie 6\")\n",
    "# Calculate gradient of y w.r.t x\n",
    "y.backward()\n",
    "print(f\"gradient of y w.r.t x is: {x.grad.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7d99a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = sin(x^2). Here x=4.0, so dy/dx = cos(x^2)*2x, ie -7.661275842587077\n",
      "Gradient of z w.r.t as per autograd: -7.661275863647461\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "x = torch.tensor([4.0], requires_grad = True)\n",
    "y = torch.pow(x,2)\n",
    "z = torch.sin(y)\n",
    "print(f\"y = sin(x^2). Here x={x.item()}, so dy/dx = cos(x^2)*2x, ie {math.cos(x**2)*2*x.item()}\")\n",
    "\n",
    "# Calculate gradients of z w.r.t its parameters\n",
    "z.retain_grad()\n",
    "y.retain_grad()\n",
    "z.backward()\n",
    "\n",
    "print(f\"Gradient of z w.r.t as per autograd: {x.grad.item()}\")\n",
    "# both are almost same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e505a989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Z00BHZZ/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:485.)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "leaf node, intermediate node, root node. By default grad is stored only for leaf nodes. If we wish\n",
    "to access grad for non leaf nodes we need to do retain_grad() for all the parameters for which\n",
    "we want to access the gradient\n",
    "'''\n",
    "# Before retaining grad for y & z we get error\n",
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "323085b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.9577]), tensor([1.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After retaining grad for y and z\n",
    "y.grad, z.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd18c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWe can do all this with tensors instead of a single value.\\nNOTE: Don't forget to clear gradient before every backward calculation because they \\nkeep getting accumulated and getting added in the tensors gradient. \\nDo this: x.grad = None\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We can do all this with tensors instead of a single value.\n",
    "NOTE: Don't forget to clear gradient before every backward calculation because they \n",
    "keep getting accumulated and getting added in the tensors gradient. \n",
    "Do this: x.grad = None\n",
    "\n",
    "Disable gradient calculation during inferencing using\n",
    "with torch.no_grad():\n",
    "    'your code goes here'\n",
    "\n",
    "or using decorator before a function\n",
    "@torch.no_grad()\n",
    "def fun():\n",
    "    'code of the fuction'\n",
    "\n",
    "or you can set requires grad for all parameters to False\n",
    "tensor.requires_grad(False)\n",
    "this would be done for all params\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d37a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.), tensor(4., grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using detach()\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = (x.detach())**2\n",
    "y1 = x**2\n",
    "y, y1\n",
    "# one has grad function, one doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b916eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
